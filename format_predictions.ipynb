{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Подготавливаем данные для второго этапа предсказания"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Посмотрим на предсказания, полученные обучением на 5 отдельных моделях:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[list(['S-ORG', 'O', 'S-MISC', 'O', 'O', 'O', 'S-MISC', 'O', 'O'])\n",
      " list(['B-PER', 'E-PER']) list(['S-LOC', 'O']) ...\n",
      " list(['S-ORG', 'O', 'S-ORG', 'O']) list(['O', 'O'])\n",
      " list(['S-ORG', 'O', 'S-ORG', 'O'])]\n"
     ]
    }
   ],
   "source": [
    "data = np.load('./data/predictions.npy')\n",
    "print(data) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Предсказания у нас сейчас в виде предложений, а нужно побить на документы!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "x_train_docs, y_train_docs = docs_from_dataset('./data/conll', 'eng.train.txt', \n",
    "                                                ('words', 'pos', 'chunk', 'ne'), \n",
    "                                                ['words', 'pos', 'chunk'], sent2features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Разбиваем предсказания на документы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "predictions_as_docs = []\n",
    "for doc in y_train_docs:\n",
    "    new_pred_doc = []\n",
    "    for sent in doc:\n",
    "        new_pred_doc.append(data[index])\n",
    "        index += 1\n",
    "    predictions_as_docs.append(new_pred_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вычисляем новые признаки и добавляем их к документам:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_token_features(x_train_docs, predictions_as_docs)\n",
    "add_entity_features(x_train_docs, predictions_as_docs)\n",
    "add_super_features(x_train_docs, predictions_as_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Полученные признаки сохраняем в формате, который используется в нейросети:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Говорим энкодеру считать пустой тег за 0, тогда pad_sequence будет работать адекватно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneHotEncoder(categorical_features='all', dtype=<class 'numpy.float64'>,\n",
       "       handle_unknown='error', n_values='auto', sparse=True)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from model.config import Config\n",
    "\n",
    "config = Config()\n",
    "encoder = utils.LabelEncoder()\n",
    "\n",
    "encoder.get('O')\n",
    "\n",
    "def get_features(x_data):\n",
    "    return [\n",
    "        x_data.get('doc_token_maj', 'O'),\n",
    "        x_data.get('corp_token_maj', 'O'),\n",
    "        x_data.get('doc_entity_maj', 'O'),\n",
    "        x_data.get('doc_entity_maj', 'O'),\n",
    "        x_data.get('corp_super_maj', 'O'),\n",
    "        x_data.get('corp_super_maj', 'O')\n",
    "    ]\n",
    "    \n",
    "dataset_formatted = []\n",
    "\n",
    "for x_doc, y_doc in zip(x_train_docs, y_train_docs):\n",
    "    for x_sent, y_sent in zip(x_doc, y_doc):\n",
    "        words = []\n",
    "        tags = []\n",
    "        features = []\n",
    "        for x_data, y_token in zip(x_sent, y_sent):\n",
    "            additional_features = get_features(x_data)\n",
    "            words.append(config.processing_word(x_data['word']))\n",
    "            tags.append(config.processing_tag(y_token))\n",
    "            features.append([encoder.get(tag) for tag in additional_features])\n",
    "        dataset_formatted.append([words, features, tags])\n",
    "        \n",
    "only_features = [sent[1] for sent in dataset_formatted]\n",
    "only_features_flat = []\n",
    "\n",
    "for sent_features in only_features:\n",
    "    only_features_flat += sent_features\n",
    "\n",
    "enc = OneHotEncoder()\n",
    "enc.fit(only_features_flat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Посмотрим, как выглядят наши признаки в первом предложении:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[([25, 22], 2881), ([0, 76, 39, 76, 32, 64, 31], 161), ([4, 76, 0, 40, 14, 33], 13239), ([32, 14, 59, 59], 13603), ([64, 71], 5382), ([67, 71, 47, 32, 71, 64, 64], 12252), ([26, 0, 21, 64, 21, 31, 3], 781), ([59, 14, 40, 67], 17131), ([60], 16992)], [[1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0], [2, 2, 2, 2, 3, 3], [0, 0, 0, 0, 1, 1], [0, 0, 0, 0, 4, 4], [0, 0, 0, 0, 0, 0], [2, 2, 2, 2, 2, 2], [0, 0, 0, 0, 4, 4], [0, 0, 0, 0, 3, 3]], [9, 13, 11, 13, 13, 13, 11, 13, 13]]\n"
     ]
    }
   ],
   "source": [
    "print(dataset_formatted[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Применим к ним обученный OneHotEncoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in dataset_formatted:\n",
    "    sent[1] = [enc.transform([word_features]).toarray()[0].tolist() for word_features in sent[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Проверим, как они выглядят теперь:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[([25, 22], 2881), ([0, 76, 39, 76, 32, 64, 31], 161), ([4, 76, 0, 40, 14, 33], 13239), ([32, 14, 59, 59], 13603), ([64, 71], 5382), ([67, 71, 47, 32, 71, 64, 64], 12252), ([26, 0, 21, 64, 21, 31, 3], 781), ([59, 14, 40, 67], 17131), ([60], 16992)], [[0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0], [1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0], [1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]], [9, 13, 11, 13, 13, 13, 11, 13, 13]]\n"
     ]
    }
   ],
   "source": [
    "print(dataset_formatted[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сохраним полученный результат:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./data/dataset_formatted', np.array(dataset_formatted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
